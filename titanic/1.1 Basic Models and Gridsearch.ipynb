{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Basic Modelling\n",
      "---\n",
      "`sklearn` will now happily ingest our data and `fit()`/`predict()` for us. Let's see what we can find out even just using some basic, built-in `sklearn` algorithms."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Data prep, again\n",
      "\n",
      "Set up our data again, since it's a new notebook"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function, division\n",
      "import pandas as pd\n",
      "import sklearn\n",
      "from scipy.stats import mode\n",
      "\n",
      "pd.options.display.max_rows = 11\n",
      "print(\"pandas: {}\".format(pd.__version__))\n",
      "print(\"sklearn: {}\".format(sklearn.__version__))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "pandas: 0.18.1\n",
        "sklearn: 0.16.1\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv(\"data/train.csv\")\n",
      "\n",
      "## fill age with mean\n",
      "age_mean = df.Age.mean()\n",
      "df.Age = df.Age.fillna(age_mean)\n",
      "\n",
      "## set fare based on Pclass\n",
      "fare_by_pclass = df.pivot_table(\n",
      "    values=\"Fare\", index=\"Pclass\", aggfunc=np.mean)\n",
      "def fareFunc(row):\n",
      "    if pd.isnull(row[\"Fare\"]) or 0.0 == row[\"Fare\"]:\n",
      "        return fare_by_pclass[row[\"Pclass\"]]\n",
      "    return row[\"Fare\"]\n",
      "df.Fare = df[[\"Fare\", \"Pclass\"]].apply(fareFunc, axis=1)\n",
      "\n",
      "## drop don't-cares\n",
      "df = df.drop([\"Name\", \"Ticket\", \"Cabin\"], axis=1)\n",
      "\n",
      "## fill embarked with mode and dummy it\n",
      "embarked_mode = mode(df.Embarked)[0][0]\n",
      "df.Embarked = df.Embarked.fillna(embarked_mode)\n",
      "dummies = pd.get_dummies(df.Embarked, prefix=\"Embarked\")\n",
      "df = pd.concat([df, dummies], axis=1)\n",
      "df = df.drop([\"Embarked\"], axis=1)\n",
      "\n",
      "## numerize Sex\n",
      "def numerize(df, column):\n",
      "    uniq = df[column].unique()\n",
      "    catmap = dict(zip(uniq, range(uniq.size)))\n",
      "    df[column] = df[column].map(catmap).astype(int)\n",
      "numerize(df, \"Sex\")\n",
      "\n",
      "## re-order columns and cut into train and test\n",
      "columns = df.columns.tolist()\n",
      "reordered = columns[1:2] + columns[0:1] + columns[2:]\n",
      "df = df[reordered]\n",
      "\n",
      "nhalf = int(len(df) / 2)\n",
      "train = df[:nhalf].values\n",
      "test = df[nhalf:].values\n",
      "df = df.values\n",
      "\n",
      "train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/numpy/lib/arraysetops.py:200: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
        "  flag = np.concatenate(([True], aux[1:] != aux[:-1]))\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "array([[   0.,    1.,    3., ...,    0.,    0.,    1.],\n",
        "       [   1.,    2.,    1., ...,    1.,    0.,    0.],\n",
        "       [   1.,    3.,    3., ...,    0.,    0.,    1.],\n",
        "       ..., \n",
        "       [   0.,  443.,    3., ...,    0.,    0.,    1.],\n",
        "       [   1.,  444.,    2., ...,    0.,    0.,    1.],\n",
        "       [   1.,  445.,    3., ...,    0.,    0.,    1.]])"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i, col in enumerate(reordered): print(i, col)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 Survived\n",
        "1 PassengerId\n",
        "2 Pclass\n",
        "3 Sex\n",
        "4 Age\n",
        "5 SibSp\n",
        "6 Parch\n",
        "7 Fare\n",
        "8 Embarked_C\n",
        "9 Embarked_Q\n",
        "10 Embarked_S\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Models\n",
      "---\n",
      "\n",
      "Let's start with something right out of the box, `SVC`, and let's start by just looking at a single column, namely, `Sex`.\n",
      "\n",
      "  - `svm` = support vector machine\n",
      "  - `SVC` = Support Vector Classification - since we are \"classifying\" the result into a given label (`0=dead, 1=alive`)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Making a `model` object is simple - you just create an instance of the class, with some model-specific parameters. I don't know what `gamma` and `C` are just yet; this is just from the `sklearn` tutorial."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC\n",
      "model = SVC(gamma=.001, C=100.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fitting a model is absurdly simple: you call `.fit()` on the training data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## our input is just the Sex column, column 3. But, need it to be 2D\n",
      "## shaped, so we grab the slice [3:4] which grabs what we want, but in\n",
      "## the correct shape.\n",
      "## our outcome is, now conveniently, at column 0, and this outcome is\n",
      "## a single value, so it's a 1D array, hence no need for slice [0:1]\n",
      "model.fit(train[0:,3:4], train[0:, 0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,\n",
        "  gamma=0.001, kernel='rbf', max_iter=-1, probability=False,\n",
        "  random_state=None, shrinking=True, tol=0.001, verbose=False)"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Predicting, and checking the accuracy of the prediction, is also super simple: you call `.predict()`.\n",
      "\n",
      "The accuracy is reasonable to report here because our `outcome` variable is categorical (Survived = dead or alive), so we just compare. If we had a continuous outcome variable, we would probably want something different, to measure the differences between predictions and ground truths."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "## our input vectors are still just the sex column\n",
      "predicted = model.predict(test[0:, 3:4])\n",
      "## compare ground truth (Survived column) against our predictions\n",
      "metrics.accuracy_score(test[0:, 0], predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "0.773542600896861"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looking only at `Sex` gets more than $3/4$ correct, which doesn't sound too bad for such a simple first pass.\n",
      "\n",
      "What if we just use all the columns at once?\n",
      "\n",
      "Note - the first argument to `.fit()` shouldn't include `Survived` (column `0`), and we also don't care about `PassengerId` (column `1`). The initial `0:,` is for axes, I think."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = SVC(gamma=.001, C=100.0)\n",
      "model.fit(train[0:, 2:], train[0:, 0])\n",
      "predicted = model.predict(test[0:, 2:])\n",
      "metrics.accuracy_score(test[0:, 0], predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "0.77130044843049328"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Well, that's not comforting; we added more information, and our prediction got slightly **worse**. Maybe `SVC` isn't the best algorithm; maybe `Sex` is just the overwhelming indicator and more data only serves to introduce noise and/or overfitting."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Model, Abstracted\n",
      "\n",
      "`sklearn` is nice enough that all of the above should generally work for algorithms that take in a rectangular, numerical array, and output a classification (in this case, binary value). We can bundle some of this a little more so it will be easier to run a model in the future.\n",
      "\n",
      "Another `model` that fits our paradigm is `RandomForestClassifier`, so let's add that to the mix."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "def getAccuracy(model, train, test):\n",
      "    model.fit(train[0:, 2:], train[0:, 0])\n",
      "    predicted = model.predict(test[0:, 2:])\n",
      "    accuracy = metrics.accuracy_score(test[0:, 0], predicted)\n",
      "    ## lame hack to get reasonable printable name for model\n",
      "    cls = str(model.__class__).replace(\"'>\",\"\")\n",
      "    cls = cls[cls.rindex(\".\")+1:]\n",
      "    print(cls, accuracy)\n",
      "\n",
      "svc = SVC(gamma=.001, C=100.0)\n",
      "rfc = RandomForestClassifier(n_estimators=100)\n",
      "for model in (svc, rfc):\n",
      "    getAccuracy(model, train, test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "SVC 0.77130044843\n",
        "RandomForestClassifier"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.784753363229\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Gridsearch\n",
      "---\n",
      "\n",
      "Alright, well, `RandomForestClassifier` seems to do slightly better - maybe `1%` or so (results will vary because it does use some `random` in there, whereas `SVC` is deterministic, so it should always be the same).\n",
      "\n",
      "But, one thing mentioned earlier is that we just arbitrarily picked some of the parameters for these algorithms - there are many more options than I listed, and currently I'm just using the default.\n",
      "\n",
      "One way to examine what the best parameter mix might be is to do a `GridSearch` over the parameter space (or, some subset of the parameter space, depending on time and compute resources).\n",
      "\n",
      "With `RandomForestClassifier`, there are some parameters that seem reasonable to try to tune - the number of estimators, the maximum number of features a tree can use, and the maximum depth of the tree.\n",
      "\n",
      "`sklearn` API to the rescue again - we set up our `GridSearchCV` object, and then just call `fit()` on it as if it were a single model - it will fit all combinations of parameter tunings (so be careful not to make it exponentially large to search) and provide attributes to return the best model.\n",
      "\n",
      "*NB*: we will use all the data here, because `GridSearch` will do the cutting into `train` and `test` for us; no need to limit ourselves to a subset of our training data.\n",
      "\n",
      "*NB*: this can take some time, even with moderate-seeming sets of parameters to tune, so `verbose=2` will print out enough info to let you know things are still working."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import GridSearchCV"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## RandomForestClassifier has:\n",
      "##  max_features (decimal)\n",
      "##  max_depth (int/float or None)\n",
      "grid = dict(\n",
      "    n_estimators = [20*i for i in range(1, 5)],\n",
      "    max_features = [i/4.0 for i in range(1,4)],\n",
      "    max_depth = [3., 4., 5.]\n",
      ")\n",
      "\n",
      "model = RandomForestClassifier(n_jobs=-1) ## -1 = parallelize\n",
      "## cv = cross validation; the number of different shufflings to try\n",
      "grid_search = GridSearchCV(model, grid, cv=5, verbose=2)\n",
      "grid_search.fit(df[0:, 2:], df[0:, 0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.3s\n",
        "[Parallel(n_jobs=1)]: Done  50 jobs       | elapsed:   17.9s\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=1)]: Done 180 out of 180 | elapsed:  1.2min finished\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "GridSearchCV(cv=5, error_score='raise',\n",
        "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
        "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
        "            min_samples_leaf=1, min_samples_split=2,\n",
        "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
        "            oob_score=False, random_state=None, verbose=0,\n",
        "            warm_start=False),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'max_features': [0.25, 0.5, 0.75], 'n_estimators': [20, 40, 60, 80], 'max_depth': [3.0, 4.0, 5.0]},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=1)"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we grab the top score and parameters, just to see what they look like, and let's re-run our model with those parameters (realizing that we will get slightly different results, since it's a `RandomForestClassifier`)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sorted(grid_search.grid_scores_,\n",
      "#    key=lambda gs: gs.mean_validation_score)\n",
      "print(grid_search.best_score_, grid_search.best_params_)\n",
      "model = RandomForestClassifier(**grid_search.best_params_)\n",
      "getAccuracy(model, train, test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.829405162738 {'max_features': 0.25, 'n_estimators': 60, 'max_depth': 5.0}\n",
        "RandomForestClassifier"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.813901345291\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Seems to perform slightly better than our original inputs - an overall improvement of something like `2-4%`."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What about `GridSearch` for `SVC`?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grid = dict(\n",
      "    kernel=[\"rbf\", \"sigmoid\"],\n",
      "    C=[10.0, 100.0]\n",
      ")\n",
      "\n",
      "model = SVC(gamma=.001)\n",
      "## cv = cross validation; the number of different shufflings to try\n",
      "grid_search = GridSearchCV(model, grid, cv=5, verbose=3, n_jobs=-1)\n",
      "grid_search.fit(df[0:, 2:], df[0:, 0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.2s\n",
        "[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:    1.3s remaining:    0.1s\n",
        "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    1.4s finished\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[CV] kernel=rbf, C=10.0 ..............................................\n",
        "[CV] kernel=rbf, C=10.0 ..............................................\n",
        "[CV] ..................... kernel=rbf, C=10.0, score=0.636872 -   0.1s[CV] ..................... kernel=rbf, C=10.0, score=0.770950 -   0.1s\n",
        "\n",
        "[CV] kernel=rbf, C=10.0 ..............................................\n",
        "[CV] kernel=rbf, C=10.0 ..............................................\n",
        "[CV] ..................... kernel=rbf, C=10.0, score=0.735955 -   0.1s[CV] ..................... kernel=rbf, C=10.0, score=0.735955 -   0.1s\n",
        "\n",
        "[CV] kernel=sigmoid, C=10.0 ..........................................\n",
        "[CV] kernel=rbf, C=10.0 ..............................................\n",
        "[CV] ................. kernel=sigmoid, C=10.0, score=0.519553 -   0.1s[CV] ..................... kernel=rbf, C=10.0, score=0.734463 -   0.1s\n",
        "\n",
        "[CV] kernel=sigmoid, C=10.0 ..........................................\n",
        "[CV] kernel=sigmoid, C=10.0 ..........................................\n",
        "[CV] ................. kernel=sigmoid, C=10.0, score=0.491620 -   0.1s[CV] ................. kernel=sigmoid, C=10.0, score=0.544944 -   0.1s\n",
        "\n",
        "[CV] kernel=sigmoid, C=10.0 ..........................................\n",
        "[CV] kernel=sigmoid, C=10.0 ..........................................\n",
        "[CV] ................. kernel=sigmoid, C=10.0, score=0.505618 -   0.1s[CV] ................. kernel=sigmoid, C=10.0, score=0.514124 -   0.1s\n",
        "\n",
        "[CV] kernel=rbf, C=100.0 .............................................\n",
        "[CV] kernel=rbf, C=100.0 .............................................\n",
        "[CV] .................... kernel=rbf, C=100.0, score=0.770950 -   0.2s[CV] .................... kernel=rbf, C=100.0, score=0.804469 -   0.2s\n",
        "\n",
        "[CV] kernel=rbf, C=100.0 .............................................\n",
        "[CV] kernel=rbf, C=100.0 .............................................\n",
        "[CV] .................... kernel=rbf, C=100.0, score=0.780899 -   0.2s[CV] .................... kernel=rbf, C=100.0, score=0.769663 -   0.2s\n",
        "\n",
        "[CV] kernel=rbf, C=100.0 .............................................\n",
        "[CV] kernel=sigmoid, C=100.0 .........................................\n",
        "[CV] .................... kernel=rbf, C=100.0, score=0.819209 -   0.2s[CV] ................ kernel=sigmoid, C=100.0, score=0.508380 -   0.1s\n",
        "\n",
        "[CV] kernel=sigmoid, C=100.0 .........................................\n",
        "[CV] kernel=sigmoid, C=100.0 .........................................\n",
        "[CV] ................ kernel=sigmoid, C=100.0, score=0.488764 -   0.1s[CV] ................ kernel=sigmoid, C=100.0, score=0.486034 -   0.1s\n",
        "\n",
        "[CV] kernel=sigmoid, C=100.0 .........................................\n",
        "[CV] ................ kernel=sigmoid, C=100.0, score=0.528090 -   0.1s\n",
        "[CV] kernel=sigmoid, C=100.0 .........................................\n",
        "[CV] ................ kernel=sigmoid, C=100.0, score=0.502825 -   0.1s\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "GridSearchCV(cv=5, error_score='raise',\n",
        "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,\n",
        "  gamma=0.001, kernel='rbf', max_iter=-1, probability=False,\n",
        "  random_state=None, shrinking=True, tol=0.001, verbose=False),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=-1,\n",
        "       param_grid={'kernel': ['rbf', 'sigmoid'], 'C': [10.0, 100.0]},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=3)"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(grid_search.best_score_, grid_search.best_params_)\n",
      "model = SVC(gamma=.001, **grid_search.best_params_)\n",
      "print(model)\n",
      "getAccuracy(model, train, test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.789001122334 {'kernel': 'rbf', 'C': 100.0}\n",
        "SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,\n",
        "  gamma=0.001, kernel='rbf', max_iter=-1, probability=False,\n",
        "  random_state=None, shrinking=True, tol=0.001, verbose=False)\n",
        "SVC"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.77130044843\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I'm not really sure why the `best_score_` is not the same as re-running the model with the `best_parameters_`; something to investigate another time."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}